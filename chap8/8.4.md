## 8.4 어텐션에 관한 남은 이야기  

***
### 8.4.1 양방향 RNN  
* 우리는 보통 글을 왼쪽에서 오른쪽으로 읽는데 전체적인 균형을 생각한다면 단어의 주변 정보를 균형있게 담고 싶어 한다.  
* 양방향 LSTM : 지금까지의 LSTM 계층에 역방향으로 처리하는 LSTM 계층도 추가한다. -> 두 LSTM 계층의 은닉 상태를 연결시킨 벡터를 최종 은닉상태로 한다.  
* 구현 방법  
    1. 2개의 LSTM 계층 사용해서 각각 계층에 주는 단어 순서 조정  
    2. LSTM 계층에는 입력문장의 순서들을 반대 순서로 나열한다.  
    
***
### 8.4.2 Attention 계층 사용 방법  
* Attention 계층 위치를 옮길 수 있다.  
* Attention 계층의 출력이 다음 시각의 LSTM 계층에 입력되도록 연결  
* LSTM 계층이 맥락 벡터의 정보 이용 가능  

***
### 8.4.3 seq2seq 심층화와 skip 연결  
1. LSTM 계층 더 깊게 쌓는 방법  
2. 주로 Encoder와 Decoder에서 같은 층수의 LSTM 계층을 이용하는 것이 일반적이다.  
3. skip 연결 : 계층을 건너뛰는 연결  
    * skip 연결 시 연결의 접속부에서는 2개의 출력이 더해진다. 
    * 덧셈은 역전파 기울기를 그대로 흘려 보내기 때문에 기울기가 아무 영향을 받지 않고 모든 계층으로 흐른다.  
    * 층이 깊어져도 기울기가 소실되지 않고 전파된다.  
    