## 3.2 단순한 word2vec  
* CBOW : continuous bag-of-words  

***
### 3.2.1 CBOW 모델의 추론 원리  
1. CBOW : 맥락(=주변 단어)으로부터 타깃(=중앙 단어) 추측하는 용도의 신경망  
    * 입력 : 맥락  
    
2. CBOW 모델 신경망 구조 : 입력층 2개, 은닉층 거쳐 출력층에 도달  
    * 입력층 -> 은닉층 : W_in 이 처리  
    * 은닉층 -> 출력층 : W_out 이 처리  
    * 입력층이 2개인 이유: 맥락으로 고려할 단어 2개 정함 (맥락 단어가 N개라면 N 개 입력층)   
    * 은닉층 : 입력층이 여러개면 전체를 평균한 값이 저장된다.  
    * 출력층 : 뉴런 하나하나가 각각의 단어에 대응하고 각 단어의 점수를 의미한다.  -> softmax -> 확률  
    
3. 입력층 -> 은닉층 : 완전연결계층에 의해 변환됨. 이때 가중치(W_in) 가 단어의 분산 표현의 정체  
    * 학습을 진행할수록 맥락에서 출현하는 단어를 잘 추측하는 방향으로 분산표현들이 갱신된다.  
    
4. '계층' 관점에서 CBOW 모델: (((you)*W_in) + ((goodbye)*W_in)) /2 -> * W_out -> score  
    
***
### 3.2.2 CBOW 모델의 학습  
* 출력층 점수 -> softmax -> 맥락이 주어졌을 때 그 중앙에 어떤 단어가 출현하는지 나타낸다.  
* CBOW 모델 학습에서는 올바른 예측을 할 수 있도록 가중치 조정하는 일을 한다.  
* 확률과 정답 레이블로부터 교차 엔트로피 오차를 구한 후 그 값을 손실로 사용해 학습 진행.  
* CBOW 모델에 Softmax 계층과 Cross Entropy Error 계층 추가  = Softmax with Loss  

***
### 3.2.3 word2vec 의 가중치와 분산 표현  
* word2vec : W_in 의 각 행이 각 단어의 분산 표현  
* 입력층의 가중치만 사용한다.. 가 가장 대중적인 선택  

