## 8.5 어텐션 응용  
***
### 8.5.1 구글 신경망 기계 번역 (GNMT)  
* 신경망 기계 번역 (Neural Machine Translation : NMT)  
    * LSTM 계층의 다층화  
    * 양방향 LSTM (encoder 첫 계층)  
    * skip 연결  
    * 다수 GPU 분산 학습 수행  
    
***
### 8.5.2 트랜스포머  
* RNN 단점 : 병렬 처리 -> RNN 없애는 연구  
* Transformer : RNN 이 아닌 어텐션 사용  
    * self attention 기술 사용 : 하나의 시계열 데이터 내에서 각 원소가 다른 원소들과 어떻게 관련되는지를 살펴보기  
    * self attention은 두 입력이 모두 동일한 시계열 데이터다.  
    
***
### 8.5.3 뉴럴 튜링 머신 (NTM)  
* 외부 메모리 사용한 확장  
* Encoder가 필요한 정보를 메모리에 쓰고, Decoder가 그 메모리로부터 필요한 정보를 읽어들인다.  
* RNN 외부에 정보 저장용 메모리 기능을 배치하고, 어텐션을 이용해 그 메모리로부터 필요한 정보를 읽거나 쓰는 방법  
* 뉴럴 튜링 머신 (NTM)