# chap8 어텐션  
## 8.1 어텐션 구조  
* 어텐션 메커니즘 : 우리에게 필요한 정보에만 '주목' 할 수 있다.  

***
### 8.1.1 seq2seq의 문제점  
* seq2seq 에서 Encoder 가 시계열 데이터를 인코딩 하고 이 정보를 Decoder로 보냄.  
* Encoder 의 출력은 '고정 길이 벡터'  
* 고정 길이 벡터 : 문장 길이에 관계없이 항상 같은 길이의 벡터로 변환 -> 길어지면 한계  

***
### 8.1.2 Encoder 개선  
* so far : LSTM 계층의 마지막 은닉 상태만 Decoder 에 전달 but 출력길이는 입력 문장의 길이에 따라 바꿔주는 것이 좋다.    
* 개선 : 시각별 LSTM 계층의 은닉 상태 벡터를 모두 사용 -> 입력된 단어와 같은 수의 벡터를 얻을 수 있다.  
* 예를 들어 5개 단어라면 5개 벡터 출력  
* LSTM 은닉 상태의 '내용' : 직전에 입력된 단어에 대한 정보가 많이 포함되어 있음  

***
### 8.1.3 Decoder 개선 1  
* Encoder에서 각 단어에 대응하는 LSTM 계층의 은닉 상태 백터를 hs로 모아 출력 -> hs 가 Decoder에 전달되어 시계열 변환이 이루어 진다.  
* 원래 : hs의 마지막줄만 빼서 Decoder에 전달했음.  
* 얼라이먼트 : 단어의 대응관계를 나타내는 정보 -> 어텐션 기술로 자동화 가능해짐  
* 어텐션 : '도착어 단어' 와 대응 관계에 있는 '출발어 단어'의 정보 골라 내는것, 이를 이용해 번역 수행  
* decoder에 '어떤 계산' 추가  
    * 입력 : Encoder로부터 받는 hs, 시각별 LSTM 계층의 은닉 상태
    * 필요한 정보만 골라 위쪽 Affine 계층으로 출력
    * Encoder의 마지막 은닉 상태 벡터는 Decoder의 첫 번째 LSTM 계층으로 전달한다.  
    
* 하고싶은 일 : 단어들의 얼라인먼트 추출 -> 각 시각에서 Decoder에 입력된 단어와 대응관계인 단어의 벡터를 hs에서 골라낸다.  
    * 예 : Decoder 가 'I' 를 출력할 때 이에 대응되는 '나'를 hs 에서 선택하면 된다. 
    * 어떤 계산 = '선택' 작업
    * 미분 불가능  
    
* '선택한다' 라는 작업을 미분 가능한 연산으로 대체하는 방법 : 하나를 선택하는 것이 아니라 모든 것을 선택한다. -> 각 단어의 중요도를 나타내는 '가중치' 를 별도로 계산한다.  
* 가중치 (a) : 각 원소가 0.0~1.0 사이의 스칼라 이며 모든 원소의 총 합은 1. 
    * 가중치와 각 단어의 벡터 hs 로부터 가중합을 구하여 원하는 벡터를 얻는다. = 맥락 벡터 (c)  
    
* Encoder 가 출력하는 hs와 각 단어의 가중치 a를 적당하게 작성하고 그 가중합을 구하는 구현  
```python
import numpy as np
T,H=5,4 # 시계열 길이, 은닉 상태 벡터 원소 수
hs=np.random.randn(T,H)
a=np.array([0.8, 0.1, 0.03, 0.05, 0.02]) #가중치

ar=a.reshape(5,1).repeat(4, axis=1) #다차원 배열을 4번 복사해서 새로운 다차원 배열 생성 
print(ar.shape) # (5,4)

t=hs*ar 
print(t.shape) # (5,4)

c=np.sum(t, axis=0)
print(c.shape) #(4,)
```
* 미니배치 처리용 가중합 구하는 구현  
```python
import numpy as np
N, T,H=10, 5,4 # 시계열 길이, 은닉 상태 벡터 원소 수
hs=np.random.randn(N,T,H)
a=np.random.randn(N,T)
ar=a.reshape(N,T,1).repeat(H, axis=2) 
# ar=a.reshape(N,T,1) #브로드캐스트 사용시

t=hs*ar 
print(t.shape) # (10,5,4)

c=np.sum(t, axis=1)
print(c.shape) #(10,4)
```
* 역전파 : repeat 의 역전파는 Sum 이고 Sum 의 역전파는 repeat이다. 
* 계산 그래프 구현  
```python
import numpy as np

class WeightSum:
    def __init__(self):
        self.params, self.grads=[],[]
        self.cache=None
    def forward(self, hs, a):
        N,T,H=hs.shape
        ar=a.reshape(N,T,1).repeat(H, axis=2)
        t=hs*ar
        c=np.sum(t, axis=1)
        self.cache=(hs, ar)
        return c
    def backward(self, dc):
        hs, ar=self.cache
        N,T,H=hs.shape
        dt=dc.reshapte(N,1,H).repeat(T, axis=1)
        dar=dt*hs
        dhs=dt*ar
        da=np.sum(dar, axis=2)
        return dhs, da
        
```

***
### 8.1.4 Decoder 개선 2  
* a(가중치) : 계산하기  
* Decoder의 LSTM 계층의 은닉 상태 벡터를 h라고 하는데 이 h와 hs가 각 단어 벡터와 얼마나 비슷한가를 수치로 나타내는 것이 목표다.  
* 이를 위해 벡터의 '내적' 사용! : 내적을 통해 hs의 각 행과 h의 유사도를 산출한다. -> 내적값(s)를 정규화하기 위해 softmax를 사용한다.  
* softmax : 각 원소가 0.0~1.0 사이가 되고, 모든 원소의 총 합이 1이 된다.  
```python
import sys
sys.path.append('..')
from common.layers import Softmax
import numpy as np

N,T,H=10,5,4
hs=np.random.randn(N,T,H)
h=np.random.randn(N,H)
hr=h.reshape(N,1,H).repeat(T, axis=1)

t=hs*hr
print(t.shape) # (10,5,4)

s=np.sum(t,axis=2)
print(s.shape) #(10,5)

softmax=Softmax()
a=softmax.forward(s)
print(a.shape) #(10,5)
```

* 계산 그래프 처리 클래스  
```python
import sys
sys.path.append('..')
from common.np import *
from common.layers import Softmax

class AttentionWeight:
    def __init__(self):
        self.params, self.grads=[],[]
        self.softmax=Softmax()
        self.cache=None
    def forward(self, hs, h):
        N,T,H=hs.shape
        hr=h.reshape(N,1,H).repeat(T, axis=1)
        t=hr*hs
        s=np.sum(t, axis=2)
        a=self.softmax.forward(s)
        self.cache=(hs, hr)
        return a
    def backward(self, da):
        hs, hr=self.cache
        N,T,H=hs.shape
        
        ds=self.softmax.backward(da)
        dt=ds.reshape(N,T,1).repeat(H, axis=2)
        dhs=dt*hr
        dhr=dt*hs
        dh=np.sum(dhr, axis=1)
        return dhs, dh
        
```

***
### 8.1.5 Decoder 개선 3  
* Attention Weight + Weight Sum 결합  
* Encoder 가 건네주는 정보 hs에서 중요한 원소에 주목하여, 그것을 바탕으로 맥락 벡터를 구해 위쪽 계층으로 전파한다. 
