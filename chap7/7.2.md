## 7.2 seq2seq  
* 
***
### 7.2.1 seq2seq 의 원리  
* Encoder-Decoder 모델  
* Encoder : 입력 데이터 인코딩(부호화), Decoder : 인코딩된 데이터 디코딩 (복호화)  
1. 인코딩  
    * 인코더는 RNN을 이용해 시계열 데이터를 h 라는 은닉 상태 벡터로 변환한다.  
    * RNN : LSTM, 단순한 RNN, GRU 사용 가능  
    * 인코더가 출력하는 h 는 고정 길이 벡터
    
2. 디코딩  
    * LSTM 계층이 벡터 h 를 입력받는다.  
    
* LSTM 의 은닉 상태가 Encoder와 Decoder를 연결해주는 '가교' 가 된다.  

***
### 7.2.2 시계열 데이터 변환용 장난감 문제  
* 문장을 단어 단위가 아닌 '문자' 단위로 분할  

***
### 7.2.3 가변 길이 시계열 데이터  
* 덧셈 문장 : 가변 길이 -> 샘플마다 데이터 시간 방향 크기가 다르다.  
* 미니배치 학습시 다수의 샘플을 한꺼번에 처리하기 때문에 한 미니배치에 속한 샘플들의 데이터 현상이 모두 같아야 한다.  
* sol : 패딩 (padding) : 원래 데이터에 의미없는 데이터를 채워 모든 데이터의 길이를 규일하게 맞추는 기법  
* softmax with loss 계층에 마스크 기능 추가해 해결 가능  

***
### 7.2.4 덧셈 데이터셋  
* dataset/addition.txt 에 담겨져 있음.  
* dataset/sequence.py : 학습 데이터를 파이썬에서 쉽게 처리할 수 있는 전용 모듈 : load_data(), get_vocab()  
    * load_data() : file_name 으로 지정한 텍스트 파일을 읽어 텍스트를 문자 id로 변환하고 훈련 데이터와 테스트 데이터로 나눠 반환  
    * get_vocab() : 문자와 문자 id의 대응관계를 담은 딕셔너리 반환  
    
```python
import sys
sys.path.append('..')
from dataset import sequence

(x_train, t_train) , (x_test, t_test) =\ sequence.load_data('addition.txt', seed=1984)
char_to_id, id_to_char = sequence.get_vocab()

print(x_train.shape, t_train.shape) # (45000,7) (45000, 5)
print(x_test.shape, t_test.shape) # (5000, 7) (5000, 5)

print(x_train[0]) # [3 0 2 0 0 11 5]
print(t_train[0]) # [6 0 11 7 5]

print(''.join([id_to_char[c] for c in x_train[0]])) # 71+118
print(''.join([id_to_char[c] for c in t_train[0]])) # _189

```