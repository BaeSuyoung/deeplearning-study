# chap6 게이트가 추가된 RNN  
* RNN : 순환 경로를 포함해 과거 정보 기억 가능
* 성능 문제 : 시계열 데이터에서 시간적으로 멀리 떨어진, 장기 의존 관계 학습 어려움.  
* Solve : LSTM, CRU 계층이 쓰인다.  (LSTM) -> 게이트 구조 (장기 의존 관계 학습 가능)  

## 6.1 RNN 문제점  
* 장기 의존 관계 학습하기 어려움.  
* BPTT 에서 기울기 소실 OR  기울기 폭발  
***
### 6.1.1 RNN 복습
* RNN계층은 시계열 데이터인 X_t 를 입력하면 h_t를 출력한다.  
* h_t : RNN계층의 은닉상태, 과거 정보 저장  
***
### 6.1.2 기울기 소실 또는 기울기 폭발  
* 기울기는 원래 학습해야 할 의미있는 정보가 들어있고 그것을 과거로 전달함으로써 장기 의존 관계를 학습한다.  
* 기울기가 중가에 사라지면 가중치 매개변수는 전혀 갱신되지 않는다.  
* 기본 RNN 계층에서는 시간을 거슬러 올라갈수록 기울기가 작아지거나 커질 수 있다.  
***
### 6.1.3 기울기 소실과 기울기 폭발의 원인  
* 정답 레이블에서 역전파로 전해지는 기울기는 tanh -> + -> Matmul 연산을 통과한다.  
1. '+' : 기울기 변화 없음  
2. 'tanh' : 기울기가 계속 작아진다.  
3. 'Matmul' : 매번 똑같은 가중치 W_h 가 사용된다.  
   * (1보다 크면) 지수적으로 증가한다. -> 기울기 폭발!!  = 오버플로우  
    * (1보다 작으면) 지수적으로 감소한다. -> 기울기 소실!!  
    

***
### 6.1.4 기울기 폭발 대책  
1. 기울기 폭발  
    * 기울기 클리핑 (gradients clipping) : threshold(문턱값) 넘을 때 기울기 수정  
    * 구현
    ```python
    import numpy as np
    
    dW1=np.random.rand(3,3)*10
    dW2=np.random.rand(3,3)*10
    grads=[dW1, dW2]
    max_norm=5.0
    
    def clip_grads(grads, max_norm):
        total_norm=0
        for grad in grads:  
            total_norm += np.sum(grad**2)
            
        total_norm=np.sqrt(total_norm)
        
        rate=max_norm / (total_norm + 1e-6)
        if rate <1 :
            for grad in grads:
                grad *= rate
    ```
   
