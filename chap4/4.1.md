# chap4 word2vec 속도 개선  
* CBOW 모델 단점 : 말뭉치 어휘 수가 많아지면 계산량이 커진다.  
* 속도 개선 : Embedding 계층 도입, 네거티브 샘플링
## 4.1 word2vec 개선 1  
* CBOW 모델 : 단어 2개 맥락으로 사용해 하나의 단어 (타깃) 추측  
* 어휘 수가 많아지면  
    1. 입력층의 원핫 표현과 가중치 행렬 W_in 의 곱 계산  -> Embedding 계층 도입
    2. 은닉층과 가중치 행렬 W_out 의 곱 및 Softmax 계층의 계산 -> 네거티브 샘플링  
    
***
### 4.1.1 통계 기반 기법의 문제점  
* word2vec에서 단어를 원핫 표현으로 바꾸고 MatMul 계층에 입력후 가중치와 곱했음. -> but 결과적으로 행렬의 특정 행을 추출하는 것 뿐이다.  
    * 원핫 표현으로의 변환과  MatMul 계층의 행렬 곱 계산 필요없음.  
    * 가중치 매개변수로부터 단어 ID에 해당하는 행을 추출하는 계층 = Embedding 계층  
    * Embedding 계층에 단어 임베딩 저장  
    
***
### 4.1.2 Embedding 계층 구현  
```python
import numpy as np

class Embedding:
    def __init__(self, W):
        self.params=[W]
        self.grads=[np.zeros_like(W)]
        self.idx=None
    def forward(self, idx): # 가중치의 특정 행 뉴런 추출
        W,= self.params
        self.idx=idx
        out=W[idx]
        return out
    def backward(self,dout):
        dW,=self.grads
        dW[...]=0
        for i, word_id in enumerate(self.idx):
            dW[word_id]+=dout(i)
        #dW[self.idx]=dout
        return None
```
* idx 원소가 중복될 때 할당 문제  -> 더하기 해야 한다.